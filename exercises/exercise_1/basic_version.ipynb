{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8937d349",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "I have took out the comments we have made during the class all comments/markdowns have specifically been made to tell what I'm changing and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927b12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "350932af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "grid_height, grid_width = env.unwrapped.desc.shape\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c54723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 25000000\n",
      "Estimated time: 5.0 minutes\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "max_epsilon = 0.9\n",
    "epsilon = max_epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "decay = 0.0005\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "trainings = 50\n",
    "total_episodes = 10_000\n",
    "max_steps = 50\n",
    "print(f'Total steps: {trainings * total_episodes * max_steps}')\n",
    "print(f'Estimated time: {total_episodes * trainings / 10_000 * 6 / 60} minutes')  # just based on my laptop's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2de40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(Q, state, action, reward, next_state, alpha, gamma):\n",
    "    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248bfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for Q-values visualisation\n",
    "def summarise_best(q_table):  # removed width and height from incoming variables as the function can already access that\n",
    "    best_values = np.max(q_table, axis = 1)\n",
    "    return best_values.reshape(grid_width, grid_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c336ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_solves = []\n",
    "step_counts = []\n",
    "\n",
    "for training in range(trainings):\n",
    "    first_solve = (-1, -1)\n",
    "    min_step_count = (-1, max_steps + 1) # episode, step count\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # next action\n",
    "            if np.random.uniform(0, 1) > epsilon:\n",
    "                action = np.argmax(q_table[state, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # results\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # q_table update\n",
    "            q_table = calculate_value(q_table, state, action, reward, observation, learning_rate, gamma)\n",
    "\n",
    "            # update: state, reward\n",
    "            state = observation\n",
    "\n",
    "            if reward == 1:\n",
    "                if first_solve == (-1, -1):\n",
    "                    first_solve = (episode, step)\n",
    "                if min_step_count[1] > step:\n",
    "                    min_step_count = (episode, step)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            # end condition check\n",
    "            if done:\n",
    "                #print(f\"Step count: {step}\")\n",
    "                break\n",
    "\n",
    "        # decay\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        #print(f\"Episode {episode} done with current reward: {total_rewards}\\nEpsilon: {epsilon}\")\n",
    "\n",
    "    first_solves.append(first_solve)\n",
    "    step_counts.append(min_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cca099a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First average solve episode: 149.08, with average step count: 22\n",
      "Smallest average step count of 5.0 reached on average episode 345.16\n"
     ]
    }
   ],
   "source": [
    "# average out the results\n",
    "average_frist_solve = [0, 0]\n",
    "average_step_count = [0, 0]\n",
    "for i in range(trainings):\n",
    "    average_frist_solve[0] += first_solves[i][0]\n",
    "    average_frist_solve[1] += first_solves[i][1]\n",
    "    average_step_count[0] += step_counts[i][0]\n",
    "    average_step_count[1] += step_counts[i][1]\n",
    "\n",
    "average_frist_solve[0] = average_frist_solve[0] / trainings\n",
    "average_frist_solve[1] = average_frist_solve[1] / trainings\n",
    "average_step_count[0] = average_step_count[0] / trainings\n",
    "average_step_count[1] = average_step_count[1] / trainings\n",
    "\n",
    "print(f'First average solve episode: {average_frist_solve[0]}, with average step count: {first_solve[1]}')\n",
    "print(f'Smallest average step count of {average_step_count[1]} reached on average episode {average_step_count[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3decf6",
   "metadata": {},
   "source": [
    "based on this I will reduce my total episodes to 2_500 for speed of testing in the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b64098",
   "metadata": {},
   "source": [
    "### Concepts talk\n",
    "The setup\n",
    "- Agent: our reinforcement learning model is called the agent, this is who acts and thinks.\n",
    "- Environment: this is what our agent interracts with, the whole world (in the agent's eye).\n",
    "- State: the current, well, state of the environment, all the info about what is where and in what way. The environment is a videogame and for example a save file in that videogame (namely the current save file) would be the state.\n",
    "- Observation: the next state, if we make a move the observation of what that move does with the environment becomes the new state.\n",
    "\n",
    "The main loop\n",
    "- Rewards: our agent \"wants\" to get as much of it as it can, this is what it optimizes around. So if we choose an unoptimal award, we'll get an unoptimal agent.\n",
    "- Policy: actually this is the first one that I can't just intuitively say, so let's google... okay scoured some sites, and kind of gets described as an agents \"brain\" or how it's actions are implemented into the world. So this is basically the connector between what an agent \"thinks\" and how that \"thought\" is actually realized in the environment/state.\n",
    "\n",
    "Q-stuff\n",
    "- Q-learning: is a complex math algorithm that is responsible for the learning of the agent. It achieves this goal through constantly adjusting values through successes and failures (which it gets through positive and negative awards). I have to assume there are infinite amount of RL algorithms and this is just a simple one that works really well. Now that I know the base concepts even I could just poke at some math until it shows RL style behaviour.\n",
    "- Q-table: is the current map of the agent's knowledge. It has the exact best move for each possible state of the environment (according to the agent).\n",
    "\n",
    "Uh, smthsmth\n",
    "- Exploration vs Exploitation: this is a choice the agent makes at every move. Exploration simply means it will act (semi-)randomly, while exploitation means it simply acts based on what it \"thinks\" is the best move (Q-table)\n",
    "\n",
    "Variables:\n",
    "- gamma: how much the agent focuses on it's future rewards, usually slightly less than the current rewards (like 0.9). However more vague goals require lower values here.\n",
    "- epsilon: how wild the agent is, basically the ration between exploration and exploitation\n",
    "- decay: how quickly epsilon goes down during training (until it reaches some minimum value)\n",
    "- learning rate: how strongly it should value \"new experiences\" over older ones. Aka if it gets a reward how much this new reward poke at the values in the Q-table\n",
    "- total episodes: how many training loops, we didn't use one but an early cutoff is probably recommeded here\n",
    "- max steps: max actions taken in a single training (or episode), used to avoid infinite loops of actions clogging the system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
