{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8937d349",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "I have took out the comments we have made during the class all comments/markdowns have specifically been made to tell what I'm changing and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927b12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "350932af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "grid_height, grid_width = env.unwrapped.desc.shape\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12c54723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 6250000\n",
      "Estimated time: 1.25 minutes\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "max_epsilon = 0.9\n",
    "epsilon = max_epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "decay = 0.0005\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "trainings = 50\n",
    "total_episodes = 2_500\n",
    "max_steps = 50\n",
    "print(f'Total steps: {trainings * total_episodes * max_steps}')\n",
    "print(f'Estimated time: {total_episodes * trainings / 10_000 * 6 / 60} minutes')  # just based on my laptop's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2de40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(Q, state, action, reward, next_state, alpha, gamma):\n",
    "    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248bfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for Q-values visualisation\n",
    "def summarise_best(q_table):  # removed width and height from incoming variables as the function can already access that\n",
    "    best_values = np.max(q_table, axis = 1)\n",
    "    return best_values.reshape(grid_width, grid_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c336ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_solves = []\n",
    "step_counts = []\n",
    "\n",
    "for training in range(trainings):\n",
    "    first_solve = (-1, -1)\n",
    "    min_step_count = (-1, max_steps + 1) # episode, step count\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # next action\n",
    "            if np.random.uniform(0, 1) > epsilon:\n",
    "                action = np.argmax(q_table[state, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # results\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # q_table update\n",
    "            q_table = calculate_value(q_table, state, action, reward, observation, learning_rate, gamma)\n",
    "\n",
    "            # update: state, reward\n",
    "            state = observation\n",
    "\n",
    "            if reward == 1:\n",
    "                if first_solve == (-1, -1):\n",
    "                    first_solve = (episode, step)\n",
    "                if min_step_count[1] > step:\n",
    "                    min_step_count = (episode, step)\n",
    "\n",
    "            total_rewards += reward - 1\n",
    "\n",
    "            # end condition check\n",
    "            if done:\n",
    "                #print(f\"Step count: {step}\")\n",
    "                break\n",
    "\n",
    "        # decay\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "        #print(f\"Episode {episode} done with current reward: {total_rewards}\\nEpsilon: {epsilon}\")\n",
    "\n",
    "    first_solves.append(first_solve)\n",
    "    step_counts.append(min_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca099a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First average solve episode: 102.3, with average step count: 12\n",
      "Smallest average step count of 5.92 reached on average episode 291.52\n"
     ]
    }
   ],
   "source": [
    "# average out the results\n",
    "average_frist_solve = [0, 0]\n",
    "average_step_count = [0, 0]\n",
    "for i in range(trainings):\n",
    "    average_frist_solve[0] += first_solves[i][0]\n",
    "    average_frist_solve[1] += first_solves[i][1]\n",
    "    average_step_count[0] += step_counts[i][0]\n",
    "    average_step_count[1] += step_counts[i][1]\n",
    "\n",
    "average_frist_solve[0] = average_frist_solve[0] / trainings\n",
    "average_frist_solve[1] = average_frist_solve[1] / trainings\n",
    "average_step_count[0] = average_step_count[0] / trainings\n",
    "average_step_count[1] = average_step_count[1] / trainings\n",
    "\n",
    "print(f'First average solve episode: {average_frist_solve[0]}, with average step count: {first_solve[1]}')\n",
    "print(f'Smallest average step count of {average_step_count[1]} reached on average episode {average_step_count[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3decf6",
   "metadata": {},
   "source": [
    "with -0.01 per step it made an about 10% decrease in needed episodes for both values compared to the \"basic_version\"\n",
    "\n",
    "same outcome with -0.1 per step\n",
    "\n",
    "interestingly with a fairly wild -1 reward per step it cut first solve episode by 25% but the optimization only by 15%ish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
